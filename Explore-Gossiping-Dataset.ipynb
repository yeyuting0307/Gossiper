{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import jieba, re, time\n",
    "jieba.set_dictionary('dict.txt.big')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove punctuation\n",
    "def rm_punc(line, strip_all=True, user_punctuation = str()):\n",
    "    import re\n",
    "    if strip_all:\n",
    "        if user_punctuation != str():\n",
    "            print('[Warning]:user_punctuation ignored, if necessary, set strip_all=False')\n",
    "        rule = re.compile(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub('',line)\n",
    "    else:\n",
    "        import zhon.hanzi, string\n",
    "        punctuation = zhon.hanzi.punctuation + string.punctuation + user_punctuation\n",
    "        re_punctuation = \"[{0}]\".format(punctuation)\n",
    "        rule = re.compile(re_punctuation)\n",
    "        line = rule.sub(repl=\"\", string=line)\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ngram\n",
    "def ngram(string, n = 2, user_stopwords= list()):\n",
    "    if type(string) is not str:\n",
    "        error_type = string.__class__.__name__\n",
    "        raise TypeError('string must be str, not {0}.'.format(error_type))\n",
    "    \n",
    "    if type(n) is not int:\n",
    "        error_type = n.__class__.__name__\n",
    "        raise TypeError('n must be int, not {0}.'.format(error_type))\n",
    "    \n",
    "    if type(user_stopwords) not in (list, tuple, set, dict, None):\n",
    "        error_type = user_stopwords.__class__.__name__\n",
    "        raise TypeError('user_stopwords must be container type, not {0}.'.format(error_type))\n",
    "    \n",
    "    string_de_punc = rm_punc(string)\n",
    "    for ptr in range(len(string_de_punc)-n+1):\n",
    "        yield string_de_punc[ptr:(ptr+n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_list,freq_lower_bound = 1):\n",
    "    from pandas import DataFrame, Series\n",
    "    from collections import Counter\n",
    "    freq_table = dict(Counter(text_list).most_common())\n",
    "    freq_table_trim = {k:v for k,v in freq_table.items() if v >= freq_lower_bound} # v >= 1 indicates rm those only appear once\n",
    "    freq_table_df = DataFrame({'Freq':Series(freq_table_trim)})\n",
    "    return freq_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "txt = '為什麼 聖結石 會被酸而 這群人 不會？'\n",
    "seg = ngram(txt, n = 2)\n",
    "[i for i in seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'generator' and 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9b7a30fa7bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseg3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'generator' and 'generator'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'type'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "問答配對筆數: 418202\n"
     ]
    }
   ],
   "source": [
    "# a simple demo for loading dataset.\n",
    "qa_pairs = []\n",
    "documents = []\n",
    "latent_stop = []\n",
    "with open('data/Gossiping-QA-Dataset.txt', 'r', encoding='utf-8') as dataset:\n",
    "    for i, line in enumerate(dataset):\n",
    "        line = line.strip('\\n')\n",
    "        q,a = line.split('\\t')\n",
    "        for n in range(1, 5):\n",
    "            [latent_stop.append(s) for s in ngram(q, n)]\n",
    "        qa_pairs.append([q,a])\n",
    "print(\"問答配對筆數:\",len(qa_pairs))\n",
    "\n",
    "WordFreq = word_freq(latent_stop, freq_lower_bound= 1000)\n",
    "Que, Ans = zip(*qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ngram stopwords\n",
    "ngram_stopwords = WordFreq.loc[WordFreq.Freq > 0.01*len(qa_pairs)].index.to_list()\n",
    "keepwords = ['台灣', '中國','肥宅', '女生', '日本', '喜歡', '朋友', '自己']\n",
    "[ngram_stopwords.remove(k) for k in keepwords if k in ngram_stopwords]\n",
    "with open('stopwords.txt', 'w') as f:\n",
    "    for ns in ngram_stopwords:\n",
    "        f.write(ns)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['沒有', '的八', '的八卦', '有沒', '有沒有', '什麼', '怎麼', '不是', '是不']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read stopwords\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords= []\n",
    "    for line in f:\n",
    "        stopwords.append(re.sub(pattern = '\\n', repl= '', string= line))\n",
    "print(len(stopwords))\n",
    "stopwords[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, q in enumerate(Que):\n",
    "    q_seg = [seg for seg in jieba.cut(rm_punc(q)) if seg not in stopwords]  ## stopword建議不要有數字\n",
    "    documents.append(TaggedDocument(q_seg, [i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 情人最後難免淪為朋友?\n",
      "A: 不用擔心 連朋友都當不成的\n",
      "Q: 有沒有佛母孔雀明王的八卦？\n",
      "A: 面，之後談判，認孔雀為母,才被嗯嗯脫身出來，\n",
      "Q: 有大客車執照去當技術員是什麼概念？\n",
      "A: 開大客太累了 12hr起跳\n",
      "Q: 清境農場為什麼有辦法開星巴克跟MOS\n",
      "A: 清境的路還好吧 還沒有很難開 更往上才是難開\n",
      "Q: 是討厭中共還是討厭中國人？\n",
      "A: 中共以及替中共添磚塊的中國人跟台灣人 ^ ^\n",
      "Q: 搞個肥宅照護管理是否有搞頭\n",
      "A: 其實　我記得　國外　有\n",
      "Q: 有沒有全家什麼最好喝的八卦？\n",
      "A: 蘋果紅茶真的超好喝！\n",
      "Q: 農歷新年是不是也該砍一砍？\n",
      "A: 支持去中國化要徹底，身上有混到漢人基因的人要自殺\n",
      "Q: 欸！領帶怎麼打！教一下！\n",
      "A: 就這樣然後這樣之後那樣 就好了\n",
      "Q: 現在房子是進場好時機？\n",
      "A: 噁心天龍人又要弄臭桃園房價和物價了\n"
     ]
    }
   ],
   "source": [
    "# randomly print the qa pairs\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(qa_pairs))\n",
    "    print(\"Q:\", qa_pairs[idx][0])\n",
    "    print(\"A:\", qa_pairs[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['肥宅', '謝師宴', '坐在', '教授', '旁邊', '會', '增加', '存在', '感', '嗎']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut('肥宅謝師宴坐在教授旁邊會增加存在感嗎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('為什麼 聖結石 會被酸而 這群人 不會？',\n",
       " '為什麼慶祝228會被罵可是慶端午不會？',\n",
       " '有沒有戰神阿瑞斯的八卦?',\n",
       " '理論與實務最脫節的系',\n",
       " '為什麼PTT這麼多人看棒球',\n",
       " '為什麼達摩祖師傳那麼好看?',\n",
       " '3D小畫家有人會畫嗎？',\n",
       " '對天龍人來說宜蘭4南部還４東部',\n",
       " '機車推出uber或計程機車會怎樣',\n",
       " '台中的龍邦世貿有人跳樓?',\n",
       " '抽到海陸會被笑娘炮兵嗎',\n",
       " '國高中的國文',\n",
       " '當初太陽花沒有留下垃圾也會被捧？',\n",
       " '世界上最難聽的國歌是哪首',\n",
       " '雙A的筆電到底可不可以帶進星巴克',\n",
       " '歐陽妮妮的男友看到她爸會有感覺嗎？',\n",
       " '有沒有imgurㄉ八卦？',\n",
       " '當年的潮物搖搖筆是噱頭嗎？',\n",
       " '肥宅初夜可以賣多少？',\n",
       " '+006是什麼地方打來的電話')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Que[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人',\n",
       " '因為屈原不是台灣人，是楚國人。',\n",
       " '爵士就是阿瑞斯 男主角最後死了',\n",
       " '哪個系不脫節...你問最不脫節的簡單多了...',\n",
       " '肥宅才看棒球\\u3000系壘一堆胖子',\n",
       " '達摩從頭到尾都是被動 (別人問他問題',\n",
       " '3D小當家有人會畫嗎',\n",
       " '他國事務..',\n",
       " '載到肥宅會很痛苦',\n",
       " '曾經當過全台第一高樓，可惜不到一年')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['聖', '結石', '酸', '而', '這群']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc2Vec Failed ....\n",
    "# start_time = time.time()\n",
    "# # It takes much time ......\n",
    "# model = Doc2Vec(documents,dm=1, dm_concat=1, vector_size=50, window=2, hs=1,alpha = 1e-4, sample = 1e-5, min_count=2, workers=4)\n",
    "# # model.train(documents, total_examples=model.corpus_count, epochs=10) \n",
    "# end_time = time.time()\n",
    "# print((end_time - start_time)/60)\n",
    "\n",
    "\n",
    "# model.save('doc2vec_chatbot.model')\n",
    "# model = Doc2Vec.load(\"doc2vec_chatbot.model\")\n",
    "\n",
    "\n",
    "## 檢驗正確性，隨機（結果不唯一）挑一篇trained dataset中的文黨，用模型重新infer，\n",
    "## 再計算與trained dataset中文檔相似度，如果模型良好，相似度第一位應該就是挑出的文檔，\n",
    "## words = u\"聖結石 被酸\" --> jieba\n",
    "## infer_vector = model.infer_vector(['機車', '推出', 'uber', '或', '計程', '機車', '會', '怎樣'])\n",
    "\n",
    "# infer_vector = model.infer_vector(['猴頭菇', '起來', '像', '雞肉', '口感'],alpha=0.025, steps=100)\n",
    "\n",
    "# similar_documents = model.docvecs.most_similar([infer_vector], topn = 10)\n",
    "\n",
    "# similar_documents\n",
    "\n",
    "# for s in similar_documents:\n",
    "#     print(s)\n",
    "#     print(Que[s[0]])\n",
    "#     print(documents[s[0]])\n",
    "\n",
    "\n",
    "# model.most_similar('肥宅', topn = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
